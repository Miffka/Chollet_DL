{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 2. Validation '''\n",
    "\n",
    "# One. Hold-out validation\n",
    "\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "training_data = data[:]\n",
    "\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# At this point - tune model, retrain, evaluate, tune again...\n",
    "\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two. K-fold validation\n",
    "\n",
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "\n",
    "# for iterated k-fold validation with shuffling add here one more cycle\n",
    "\n",
    "# n_iterations = 5\n",
    "# for i in range(iterations):\n",
    "#     np.random.shuffle(data)\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold:\n",
    "                          num_validation_samples * (fold + 1)]\n",
    "    training_data = data[:num_validation_samples * fold] +\n",
    "                    data[num_validation_samples * (fold + 1):]\n",
    "        \n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validatioin_score)\n",
    "    \n",
    "validation_score = np.average(validation_scores)\n",
    "\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  How to choose evaluation protocol\n",
    " \n",
    " 1) *Data representativeness* - both test and training set are **representative** of the data in hand. \n",
    " \n",
    " 2) *The arrow of time* - if trying predict the future given the past, do not shuffle data before splitting it. Data in test set and validation set are **posterior** to the data in the training set.\n",
    " \n",
    " 3) *Redundancy in data* - if data points appear twice, then shuffling the data and splitting it into training and validation sets results in redundancy between data. Make sure that sets are disjoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing, feature engineering, and feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Vectorization '''\n",
    "\n",
    "# Turn data into tensors of float32 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Value normalization '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Value normalization '''\n",
    "\n",
    "# 1) Make the values lie in the 0-1 range for images.\n",
    "# 2) Take small values.\n",
    "# 3) Be homogenous for all features\n",
    "# 4) Normalize each feature independently to have mean of 0 and st_dev of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Handling missing values '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Handling missing values '''\n",
    "\n",
    "# With neural networks it is safe to input missing values as 0, if 0 isn't a meaningful value.\n",
    "# Make sure training data has missing values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Feature engineering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Clock arrows example '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Clock arrows example '''\n",
    "\n",
    "# If you know what data mean - make sure your NN can handle \n",
    "#  this meaning rather than row data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Overfitting and underfitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If loss on both training and validation set are diminishing\n",
    "#  from batch to batch, just keep learning.\n",
    "\n",
    "# The best solution is to get more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Reducing the network's size '''\n",
    "\n",
    "# A model with more parameters has more memorization capacity.\n",
    "\n",
    "## Original model \n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Model with lower capacity\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Model with higher capacity\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x1c13d7db38>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Adding weight regularization '''\n",
    "\n",
    "# At training time, adds 0.001 * weight_coefficient_value to the total loss\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                      activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                      activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "# Different regularizers\n",
    "\n",
    "regularizers.l1(0.001)\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Adding dropout '''\n",
    "\n",
    "# At training time, drops out 50% of the units in the output\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "\n",
    "# At test time, scale output to stay in the same range\n",
    "layer_output *= 0.5\n",
    "\n",
    "# We can also do both operation at training time\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "layer_output /= 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB network with dropout\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The universal workflow of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Defining the problem and assembling a dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What will input data be? \n",
    "\n",
    "* What type of problem will it be? Binary classification? Multiclass classification? Scalar regression? Vector regression? Multiclass multilabel classification?\n",
    "\n",
    "Always be aware of nonstationary problems. E.g. recommendation engine for clothes which was training on August data, to be used to predict clothes in winter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Choosing a measure of success*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy? ROC AUC? MAE? RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Deciding on an evaluation protocol*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maintaining a hold-out validatioin set - when you have plenty of data.\n",
    "\n",
    "* Doing K-fold cross-validation - too few samples for hold-out validation.\n",
    "\n",
    "* Doing iterated K-fold validation - when little data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Preparing your data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Format data as tensors.\n",
    "\n",
    "* Scale data to small values - [-1, 1] or [0, 1].\n",
    "\n",
    "* If different features take values in different ranges, normalize data.\n",
    "\n",
    "* Make feature engineering, especially for small-data problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Developing a model that does better than a baseline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You hypothesize that your outputs can be predicted given your inputs.\n",
    "\n",
    "* You hypothesize that the available data is sufficiently informative to learn the relationship between inputs and outputs.\n",
    "\n",
    "Three key choices to build first network:\n",
    "\n",
    "1) Last-layer activation - sigmoid, softmax or no activation.\n",
    "\n",
    "2) Loss function - binary_crossentropy, categorical_crossentropy, mse etc.\n",
    "\n",
    "3) Optimization configuration - optimizer and learning rate (rmsprop and default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Scaling up: developing a model that overfits*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Add layers\n",
    "\n",
    "2) Make the layers bigger\n",
    "\n",
    "3) Train for more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Regularizing model and tuning hyperparameters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add dropout.\n",
    "\n",
    "* Try different architectures: add of remove layers.\n",
    "\n",
    "* Add L1 and/or L2 regularization.\n",
    "\n",
    "* Try different hyperparameters (number of units per layer or learning rate).\n",
    "\n",
    "* Iterate to feature engineering: add new features or remove features that don't seem to be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
